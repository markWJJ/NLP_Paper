2017/8/9

/××××××××
L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合
详见url：http://blog.csdn.net/jinping_shi/article/details/52433975

稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。

拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象
可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。
\那为什么L2正则化可以获得值很小的参数？


最终用于迭代计算参数θ的迭代式为：
θj:=θj−α1m∑i=1m(hθ(x(i))−y(i))x(i)j(4)

其中α是learning rate. 上式是没有添加L2正则化项的迭代公式，如果在原始代价函数之后添加L2正则化，则迭代公式会变成下面的样子：
θj:=θj(1−αλm)−α1m∑i=1m(hθ(x(i))−y(i))x(i)j(5)

其中λ就是正则化参数。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的。

最开始也提到L1正则化一定程度上也可以防止过拟合。之前做了解释，当L1的正则化系数很小时，得到的最优解会很小，可以达到和L2正则化类似的效果。

L1正则化参数：通常越大的λ可以让代价函数在参数为0时取到最小值。
L2正则化参数：从公式5可以看到，λ越大，θj衰减得越快。另一个理解可以参考图2，λ越大，L2圆的半径越小，最后求得代价函数最值时各参数也会变得很小。
×××××××××/

/××××××××
一般来说，无论是C、C++、还是pas，首先要把源文件编译成中间代码文件，在Windows下也就是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做编译（compile）。然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。
 编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常是你需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。
  链接时，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。

Makefile的规则
在讲述这个Makefile之前，还是让我们先来粗略地看一看Makefile的规则。
target... : prerequisites ...
command
target也就是一个目标文件，可以是Object File，也可以是执行文件。还可以是一个标签（Label），对于标签这种特性，在后续的“伪目标”章节中会有叙述。
prerequisites就是，要生成那个target所需要的文件或是目标。
command也就是make需要执行的命令。（任意的Shell命令）


/×××××××
在神经网络的输入层注入噪声 (Sietsma and Dow, 1991) 也可以被看作是数据增强的一种方式。对于许多分类甚至一些回归任务而言,即使小的随机噪声被加到输入,任务仍应该是能够被解决的。然而,神经网络被证明对噪声不是非常健壮 (Tangand Eliasmith, 2010)。改善神经网络健壮性的方法之一是简单地将随机噪声添加到输入再进行训练。输入噪声注入是一些无监督学习算法的一部分,如去噪自编码器(Vincent et al., 2008a)。向隐藏单元施加噪声也是可行的,这可以被看作在多个抽象层上进行的数据集增强。Poole et al. (2014) 最近表明,噪声的幅度被细心调整后。

向输入添加方差极小的噪声等价于对权重施加范数惩罚  在一般情况下,噪声注入远比简单地收缩参数强大,特别是噪声被添加到隐藏单元时会更加强大。

另一种正则化模型的噪声使用方式是将其加到的权重。这项技术主要用于循环神经网络 (Jim et al., 1996; Graves, 2011)。这可以被解释为关于权重的贝叶斯推断的随机实现。贝叶斯学习过程将权重视为不确定的,并且可以通过概率分布表示这种不确定性。向权重添加噪声是反映这种不确定性的一种实用的随机方法。

例如,标签平滑(label smoothing)通过把确切分类目标从 0 和1 替换成 ε/k−1和 1 − ε,正则化具有 k 个输出的 softmax 函数 的模型。标准交叉熵损失可以用在这些非确切目标的输出上。使用 softmax 函数 和明确目标的最大似然学习可能永远不会收敛——softmax 函数 永远无法真正预测 0 概率或 1 概率,因此它会继续学习越来越大的权重,使预测更极端。使用如权重衰减等其他正则化策略
能够防止这种情况。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。这种策略自 20 世纪 80 年代就已经被使用,并在现代神经网络继续保持显著特色 (Szegedy et al., 2015)。
****/



2017/8/14
/××××
多任务学习 (Caruana, 1993) 是通过合并几个任务中的样例(可以视为对参数施加的软约束)来提高泛化的一种方式。额外的训练样本以同样的方式将模型的参数推向泛化更好的方向,当模型的一部分在任务之间共享时,模型的这一部分更多地被约束为良好的值(假设共享是合理的),往往能更好地泛化。

从深度学习的观点看,底层的先验知识如下:能解释数据变化(在与之相关联的不同任务中观察到)的因素中,某些因素是跨两个或更多任务共享的。

这意味着如果我们返回使验证集误差最低的参数设置,就可以获得更好的模型(因此,有希望获得更好的测试误差)。在每次验证集误差有所改善后,我们存储模型参数的副本。当训练算法终止时,我们返回这些参数而不是最新的参数。当验证集上的误差在事先指定的循环次数内没有进一步改善时,算法就会终止。此过程在算法 7.1 中有更正式的说明则化形式。它的流行主要是因为有效性和简单性。

××××/

/××××

目前为止,本章讨论对参数添加约束或惩罚时,一直是相对于固定的区域或点。例如,L2 正则化(或权重衰减)对参数偏离零的固定值进行惩罚。然而,有时我们可能需要其他的方式来表达我们对模型参数适当值的先验知识。有时候,我们可能无法准确地知道应该使用什么样的参数,但我们根据领域和模型结构方面的知识得知模型参数之间应该存在一些相关性。


（217）表示的正则化
表示的正则化可以使用参数正则化中同种类型的机制实现。表示的范数惩罚正则化是通过向损失函数 J 添加对表示的范数惩罚来实现的。我们将这个惩罚记作 Ω(h)。和以前一样,我们将正则化后的损失函数记作 J:J(θ;X, y) = J(θ; X, y) + αΩ(h),

 
2017/8/16
/****
对抗样本也提供了一种实现半监督学习的方法。在与数据集中的标签不相关联的点 x 处,模型本身为其分配一些标签 ŷ。模型的标记 ŷ 未必是真正的标签,但如果模型是高品质的,那么 ŷ 提供正确标签的可能性很大。我们可以搜索一个对抗样本 x′ ,导致分类器输出一个标签 y ′ 且 y′ ̸= ŷ。不使用真正的标签,而是由训练好的模型提供标签产生的对抗样本被称为 虚拟对抗样本(virtual adversarial example)

对抗样本是通过稍微修改实际样本而构造出的合成样本，以便于一个分类器以高置信度认为它们属于错误的分类。
http://www.csdn.net/article/2015-07-19/2825248
事实：到目前为止，我们已经能够为我们测试过的每一个模型生成对抗样本，包括像最邻近这样的最传统的机器学习模型。深度学习是目前为止对对抗训练最有抵抗性的技术。
我们最近的实验表明，深度模型的表现是非常线性的。线性模型在外推远离训练数据的区域有着极度的优势。这也解释了对抗性和垃圾分类样本中发生的很多错误。

注意最后一条，大家可能在想，我们手动生成对抗样本喂给分类器学习不就完了吗，但是效果并不好，因为对抗样本的生成比较expensive。如果我们能知道对抗样本产生的原因，也许能更轻松的制造对抗样本，从而训练。 

他认为模型在高维空间中的线性性是对抗样本存在的罪魁祸首

正则化会在一定程度上减少对抗样本错误率
我觉得有一种可能解释是对于strong的正则化，模型（比如说线性模型）的weight会变得比较低。记得我们举过的例子，对线性模型，精心设计的对抗样本能使模型的activation改变大约ϵmn，这里的\epsilon是样本允许的波动范围，是固定的，m是平均的weight，当m很小的时候，对activation的扰动ϵmn会很小，从而对分类的影响比较小。所以，这里问题的关键变成了为什么strong的正则化会使weight降低。还记得在cost function里面加的正则化，是对样本的扰动进行惩罚，而高weight会使样本的扰动更大，那训练的时候模型显然会自然的降低weight来减少cost function的值。所以strong的正则化会使weight降低。
http://blog.csdn.net/cdpac/article/details/53170940


然而,经验风险最小化很容易导致过拟合。高容量的模型会简单地记住训练集。在很多情况下,经验风险最小化并非真的可行。最有效的现代优化算法是基于梯度下降的,但是很多有用的损失函数,如 0 − 1 损失,没有有效的导数(导数要么为零,要么处处未定义)。这两个问题说明,在深度学习中我们很少使用经验风险最小化。反之,我们会使用一个稍有不同的方法,我们真正优化的目标会更加不同于我们希望优化的目标。

然而,经验风险最小化很容易导致过拟合。高容量的模型会简单地记住训练集。在很多情况下,经验风险最小化并非真的可行。最有效的现代优化算法是基于梯度下降的,但是很多有用的损失函数,如 0 − 1 损失,没有有效的导数(导数要么为零,要么处处未定义)。这两个问题说明,在深度学习中我们很少使用经验风险最小化。反之,我们会使用一个稍有不同的方法,我们真正优化的目标会更加不同于我们希望优化的目标。

在优化凸函数时,会遇到一些挑战。这其中最突出的是 Hessian 矩阵 H 的病态。这是数值优化、凸优化或其他形式的优化中普遍存在的问题,更多细节请回顾第 4.3.1 节。病态问题一般被认为存在于神经网络训练过程中。病态体现在随机梯度下降会‘‘卡’’ 在某些情况,此时即使很小的更新步长也会增加代价函数。


2017/8/17

/****
对于非凸函数时,如神经网络,有可能会存在多个局部极小值。事实上,几乎所有的深度模型基本上都会有非常多的局部极小值。然而,我们会发现这并不是主要问题。

除了权重空间对称性,很多神经网络还有其他导致不可辨认的原因。意整流线性网络或者  网络中,我们可以将传入权重和偏置扩大 

maxout网络:http://blog.csdn.net/hjimce/article/details/50414467

很多从业者将神经网络优化中的所有困难都归结于局部极小值。我们鼓励从业者要仔细分析特定的问题。一种能够排除局部极小值是主要问题的检测方法是画出梯度范数随时间的变化。如果梯度范数没有缩小到一个微小的值,那么该问题既不是局部极小值,也不是其他形式的临界点。在高维空间中,很难明确证明局部极小值是导致问题的原因。许多并非局部极小值的结构也具有很小的梯度。


多类随机函数表现出以下性质:低维空间中,局部极小值很普遍。在更高维空间中,局部极小值很罕见,而鞍点则很常见。对于这类函数 f : Rn → R 而言,鞍点和局部极小值的数目比率的期望随 n 指数级增长。我们可以从直觉上理解这种现象——Hessian 矩阵在局部极小点处只有正特征值。而在鞍点处,Hessian 矩阵则同时具有正负特征值。试想一下,每个特征值的正负号由抛硬币决定。在一维情况下,很容易抛硬币得到正面朝上一次而获取局部极小点。在 n-维空间中,要抛掷 n 次硬币都正面朝上的难度是指数级的。具体可以参考 Dauphin et al. (2014),它回顾了相关的理论工作。


 另一方面,实验中梯度下降似乎可以在许多情况下逃离鞍点。Goodfellow 法而言,目前情况还不清楚。鞍点附近的梯度通常会非常小。

多层神经网络通常存在像悬崖一样的斜率较大区域,如图 8.3 所示。这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时,梯度更新会很大程度地改变参数值,通常会完全跳过这类悬崖结构。不管我们是从上还是从下接近悬崖,情况都很糟糕,但幸运的是我们可以用使用第 10.11.1 节介绍的启发式 梯度截断(gradient clipping)来避免其严重的后果。其

使其不太可能走出梯度近似为最陡下降方向的悬崖区域。悬崖结构在循环神经网络的代价函数中很常见,因为这类模型会涉及到多个因子的相乘,其中每个因子对应一个时间步。因此,长期时间序列会产生大量相乘。

梯度消失与爆炸 P247

损失函数和代理损失函数;http://blog.csdn.net/google19890102/article/details/50522945


虽然随机梯度下降仍然是非常受欢迎的优化方法,但其学习过程有时会很慢。动量方法 (Polyak, 1964) 旨在加速学习,特别是处理高曲率、小但一致的梯度,或是带噪声的梯度。动量算法积累了之前梯度指数级衰减的移动平均,并且继续沿该方向移动。动量的效果如图 8.5 所示。

初始化每个单元使其和其他单元计算不同的函数。这或许有助于确保没有输入模式学习算法将一直以相同的方式更新这两个单元。即使模型或训练算法能够使用随性为不同的单元计算不同的更新(例如使用 丢失在前向传播的零空间中,没有梯度模式丢失在反向传播的零空间中

更大的初始权重具有更强的破坏对称性的作用,有助于避免冗余的单元。它们也有助于避免在每层线性成分的前向或反向传播中丢失信号——矩阵中更大的值在矩阵乘法中有更大的输出。如果初始权重太大,那么会在前向传播或反向传播中产生爆炸的值。在循环网络中,很大的权重也可能导致 混沌(chaos)(对于输入中很小的扰动非常敏感,导致确定性前向传播过程表现随机)。在一定程度上,梯度爆炸问题可以通过梯度截断来缓解(执行梯度下降步骤之前设置梯度的阈值)。较大的权重也会产生使得激活函数饱和的值,导致饱和单元的梯度完全丢失。这些竞争因素决定了权重的理想初始大小。




 有些启发式方法可用于选择权重的初始大小。一种初始化  DRAFTm  个输入和 n 输出的全连接层的权重的启发式方法是从分布 U (− √ 1 , √ 1 ) 中采样权重,而 Glorot et al.m m(2011a) 建议使用 标准初始化(normalized initialization) p256


Delta-bar-delta  DRAFT算法 (Jacobs, 1988) 是一个早期的在训练时适应模型参数各自学习率的启发式方法。该方法基于一个很简单的想法,如果损失对于某个给定模型参数的偏导保持相同的符号,那么学习率应该增加。如果对于该参数的偏导变化了符号,那么学习率应减小。当然,这种方法只能应用于全批量优化中。
 






2017/8/18


SGD方法的一个缺点是，其更新方向完全依赖于当前的batch，因而其更新十分不稳定。解决这一问题的一个简单的做法便是引入momentum。momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力： 


tensorflow 随机数
tf.random_normal(shape,mean=0.0,stddev=1.0,dtype=tf.float32) 
tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32) 
tf.random_uniform(shape,minval=0,maxval=None,dtype=tf.float32) 
这几个都是用于生成随机数tensor的。尺寸是shape 
random_normal: 正太分布随机数，均值mean,标准差stddev 
truncated_normal:截断正态分布随机数，均值mean,标准差stddev,不过只保留[mean-2*stddev,mean+2*stddev]范围内的随机数 
random_uniform:均匀分布随机数，范围为[minval,maxval]


tf.cast(x, dtype, name=None)
此函数是类型转换函数


2017/8/19

loss = tf.reduce_mean(tf.where(tf.greater(y,y_), (y - y_) * loss_more, (y_ - y) * loss_less))

tf.greater(x,y):对x,y进行比较，x>y返回true

 tf.losses.mean_squared_error(y, y_)，均方误差 损失函数（MSE）


2017/8/21

优化函数
SGD（随机梯度下降）tensorflw（GradientDescentOptimizer）：对于训练数据集，我们首先将其分成n个batch，每个batch包含m个样本。我们每次更新都利用一个batch的数据，而非整个训练集。即：xt+1=xt+Δxt  Δxt=−ηgt  其中，η为学习率，gt为x在t时刻的梯度。

Momentum（带有动量的梯度下降）tensorflw（MomentumOptimizer）：SGD方法的一个缺点是，其更新方向完全依赖于当前的batch，因而其更新十分不稳定。解决这一问题的一个简单的做法便是引入momentum。momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：
Δxt=ρΔxt−1−ηgt   其中，ρ 即momentum，表示要在多大程度上保留原来的更新方向，这个值在0-1之间，在训练开始时，由于梯度可能会很大，所以初始值一般选为0.5；当梯度不那么大时，改为0.9。η 是学习率，即当前batch的梯度多大程度上影响最终更新方向，跟普通的SGD含义相同。ρ 与 η 之和不一定为1。


Nesterov Momentum（Nesterov动量梯度下降）tensorflow（）：这是对传统momentum方法的一项改进，由Ilya Sutskever(2012 unpublished)在Nesterov工作的启发下提出的。首先，按照原来的更新方向更新一步（棕色线），然后在该位置计算梯度值（红色线），然后用这个梯度值修正最终的更新方向（绿色线）。上图中描述了两步的更新示意图，其中蓝色线是标准momentum更新路径。公式描述为：
Δxt=ρΔxt−1−ηΔf(xt+ρΔxt−1)


Adagrad（自适应率梯度下降）tensorflow（AdagradDAOptimizer）：上面提到的方法对于所有参数都使用了同一个更新速率。但是同一个更新速率不一定适合所有参数。比如有的参数可能已经到了仅需要微调的阶段，但又有些参数由于对应样本少等原因，还需要较大幅度的调动。Adagrad就是针对这一问题提出的，自适应地为各个参数分配不同学习率的算法。其公式如下： 

http://blog.csdn.net/luo123n/article/details/48239963

其中gt 同样是当前的梯度，连加和开根号都是元素级别的运算。eta 是初始学习率，由于之后会自动调整学习率，所以初始值就不像之前的算法那样重要了。而ϵ是一个比较小的数，用来保证分母非0。其含义是，对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。核心思想：对于频繁出现的参数使用更小的更新速率，对于不频繁出现的参数使用更大的更新速率。正因为如此，该优化函数脚适用于稀疏的数据，

Adadelta：adadelta是adagrad的延伸，不同于adadelta将以前所有的偏导都累加起来，adadelta控制了累加的范围到一定的窗口中。
但是，并非简单的将窗口大小设置并且存储，我们是通过下式动态改变的上述的G： 


##目前最优的优化算法 adam（自适应矩估计）tensorflow（AdamOptimizer）：Adam 也是基于梯度下降的方法，但是每次迭代参数的学习步长都有一个确定的范围，不会因为很大的梯度导致很大的学习步长，参数的值比较稳定。



tensorflow提供了一个灵活的学习率设置方法，指数衰减函数tf.train.exponential_decay()

滑动平均模型对于采用GradientDescent或Momentum训练的神经网络的表现都有一定程度上的提升，原理：在训练神经网络时，不断保持和更新每个参数的滑动平均值，在验证和测试时，参数的值使用其滑动平均值，能有效提高神经网络的准确率。tf.train.ExponentialMovingAverage




2017/8/22

批标准化 (Ioffe and Szegedy, 2015) 是优化深度神经网络中最激动人心的最新创新之一。实际上它并不是一个优化算法,而是一个自适应的重参数化的方法,试图解决训练非常深的模型的困难。DRAFT


2017/8/23

在某些情况下,将一个优化问题分解成几个部分,可以更快地解决原问题。如果我们相对于某个单一变量 xi 最小化 f (x),然后相对于另一个变量 xj 等等,反复循环所有的变量,我们会保证到达(局部)极小值。这种做法被称为 坐标下降(coordinate descent),因为我们一次优化一个坐标。更一般地, 块坐标下降(blockcoordinate descent)是指对于某个子集的变量同时最小化。术语 “坐标下降’’ 通常既指块坐标下降,也指严格的单个坐标下降。

 贪心算法(greedy DRAFTalgorithm)将问题分解成许多部分,然后独立地在每个部分求解最优值。令人遗憾的是,结合各个最佳的部分不能保证得到一个最佳的完整解。输出完成其目标和预测教师网络的中间层。尽管一个窄而深的网络似乎比宽而浅的网络更难训练,但窄而深网络的泛化能力可能更好,并且如果其足够窄,参数足够少,那么其计算代价更小。
 
在实践中,选择一族容易优化的模型比使用一个强大的优化算法更重要。



2017/8/30

tensorflow 转置函数 transpose 
tf.unstack()　　将给定的R维张量拆分成R-1维张量

　　将value根据axis分解成num个张量，返回的值是list类型，如果没有指定num则根据axis推断出！

2018/1/15

正式的讲，交叉熵的公式为： p_k*log_2 1/q_k ，其中 p_k 表示真实分布， q_k 表示非真实分布
KL散度：KL（p || q） = H（p，q） - H（p）= sum(p_k*log_2(p_k/q_k)) ，其中 p_k 表示真实分布， q_k 表示非真实分布
线性回归使用均方误差(Mean Squared Error, MSE)作为loss function.
交叉熵(CCE)---逻辑回归 逻辑回归使用最大似然方法估计参数.

我们希望：ANN在训练时，如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛。然而，如果使用二次代价函数(MSE)训练ANN，看到的实际效果是，如果误差越大，参数调整的幅度可能更小，训练更缓慢
误差越大，梯度就越大，参数w调整得越快，
http://blog.csdn.net/u014313009/article/details/51043064
MSE训练速度慢 当误差越大时 训练梯度越小
CCE训练速度比MSE快， 因为其 梯度比MSE而言 不需要考虑激活函数的梯度（sigimod） 而激活函数的梯度在上下两段的梯度小 因此 MSE比CCE的训练慢

梯度下降法的缺点包括（主要相比牛顿法）靠近极小值时速度减慢。直线搜索可能会产生一些问题。可能会“之字型”地下降
梯度下降收敛速度慢的原因：
1 梯度下降中，x = φ(x) = x - f'(x)，φ'(x) = 1 - f''(x) != 0极值领域一般应该不会满足为0。则根据[最优化方法：非线性方程的求极值方法]高阶收敛定理2.6可以梯度下降在根*x附近一般一阶收敛。
2 梯度下降方法中，负梯度方向从局来看是二次函数的最快下降方向，但是从整体来看却并非最好

2> 梯度下降最优解
梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的


在迭代所有训练样例时，这些权值更新的序列给出了对于原来误差函数的梯度下降的一个合理近似
–通过使下降速率的值足够小，可以使随机梯度下降以任意程度接近于真实梯度下降
批梯度下降计算梯度时，同时计算了所有数据，其中包含了相似的（一定程度上可以认为是冗余的）数据（比如说一个地点附近两个点，另一个地点附近一个点，但是真实分布可能是另一个地点附近也是两个点，这样的话，前一个地点的那两个点可以认为是冗余的），所以实际上，在批梯度下降中即使我们使用少量的随机数据得到的结果和计算所有数据的结果差不多。而随机梯度就是只使用了随机选择的数据，如果这些数据刚好不是冗余的（或者是近似这样）就可以不完全遍历完数据就到达最优解附近（一般会损失一定精度不是最优的而是附近

批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。

随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况

f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate

sigmoid 函数曾经被使用的很多，不过近年来，用它的人越来越少了。主要是因为它的一些 缺点：

    Sigmoids saturate and kill gradients. （saturate 这个词怎么翻译？饱和？）sigmoid 有一个非常致命的缺点，当输入非常大或者非常小的时候（saturation），这些神经元的梯度是接近于0的，从图中可以看出梯度的趋势。所以，你需要尤其注意参数的初始值来尽量避免saturation的情况。如果你的初始值很大的话，大部分神经元可能都会处在saturation的状态而把gradient kill掉，这会导致网络变的很难学习。
    Sigmoid 的 output 不是0均值. 这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 
    产生的一个结果就是：如果数据进入神经元的时候是正的(e.g. x>0 elementwise in f=wTx+b)，那么 w 计算出的梯度也会始终都是正的。
    当然了，如果你是按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的 kill gradients 问题相比还是要好很多的


 tanh 是上图中的右图，可以看出，tanh 跟sigmoid还是很像的，实际上，tanh 是sigmoid的变形： 
tanh(x)=2sigmoid(2x)−1与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好（毕竟去粗取精了嘛）但是它还是存在梯度饱和的问题

 ReLU 的优点： Krizhevsky et al. 发现使用 ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多(看右图)。有人说这是因为它是linear，而且 non-saturating
相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。
ReLU 的缺点： 当然 ReLU 也有缺点，就是训练的时候很”脆弱”，很容易就”die”了. 什么意思呢？
 举个例子：一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。
如果这个情况发生了，那么这个神经元的梯度就永远都会是0.实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都”dead”了。 
当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。

Maxout的拟合能力非常强，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个maxout节点就可以拟合任意的凸函数了(相减)，前提是”隐含层”节点的个数可以任意多

卷积是使用tf.nn.conv2d, 池化使用tf.nn.max_pool
def conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None,
           data_format=None, name=None):
def max_pool(value, ksize, strides, padding, data_format="NHWC", name=None):


 AdaBoost 是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器，即弱分类器，然后把这些弱分类器集合起来，构造一个更强的最终分类器



Hinge Loss是一种目标函数（或者说损失函数）的名称，有的时候又叫做max-margin objective。其最著名的应用是作为SVM的目标函数。其二分类情况下，公式如下：l(y)=max(0,1−t⋅y)其中，y是预测值（-1到1之间），t为目标值（±1）。

实际应用中，一方面很多时候我们的y的值域并不是[-1,1]，比如我们可能更希望y更接近于一个概率，即其值域最好是[0,1]。另一方面，很多时候我们希望训练的是两个样本之间的相似关系，而非样本的整体分类，所以很多时候我们会用下面的公式：
l(y,y′)=max(0,m−y+y′)其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数）即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。
比如，我们想训练词向量，我们希望经常同时出现的词，他们的向量内积越大越好；不经常同时出现的词，他们的向量内积越小越好。则我们的hinge loss function可以是：l(w,w+,w−)=max(0,1−wT⋅w++wT⋅w−)


# 对话系统/微软小冰
理解用户：社交机器人必须具备同理心。它需要能够从对话中识别用户的情感，以检测情感是如何随时间而推移，因而能理解用户的情感需求。这就要求机器人能理解询问、分析用户、检测情绪、识别情感，并动态地追踪用户在对话中的情感变化

人际关系的生成：社交机器人必须展现足够多的社交技巧。因为用户可能有不同的背景、不同的个人兴趣和独特的需求，因此社交机器人必须能针对不同的用户生成个性化的对话。社交机器人需要生成情感上适当的回应、鼓励和激励，并满足用户的兴趣需求。它还需要引导对话主题，并管理人际间的关系，使用户感到有良好的理解并激发更多的交流。它同样还需要意识到不合适的信息，以免生成带偏见或冒犯用户的会话

个性：社交机器人需要呈现连贯和一致的个性，因而能获得用户的持续信任。聊天机器人的连贯个性能帮助用户设定适当的对话期望，不会太高也不会太低。个性的设定包括年龄、性别、语言、说话风格、一般看法、知识水平、专业领域和适当的口音。这些设定都会影响社交机器人对用户的反应，因此社交机器人需要通过主动学习和适应性学习改善与用户的交互。

EQ 和 IQ 的有机结合：除了闲聊外，社交机器人还需要掌握一系列技能来帮助用户完成一些特定的任务。它们需要分析用户的请求，并执行一定的推理以响应这些提问。因此社交机器人需要足够的 IQ 以对知识和记忆进行建模，对图像和语言进行理解、推理、生成和预测。这些 IQ 不仅仅是各种基础的技术，同样是构建更高级 EQ 的根本。

我们将每次会话中交流回合数（CPS）作为社交聊天机器人的度量指标。CPS 是聊天机器人和用户在一次会话中交流回合的平均数。CPS 越大，社交聊天机器人的参与度越高。

核心聊天是社交聊天机器人的核心模块。它的任务是接收用户的文本输入，然后生成一个文本响应作为输出。它提供了社交聊天机器人的交流能力。图 8 给出了核心聊天中的关键组件。












2018/1/18
tensoflow 分布式

分布式Tensorflow底层的通信是gRPCgRPC首先是一个RPC,即远程过程调用
Cluster.Job.Task

Job是Task的集合.
Cluster是Job的集合
我们介绍一下Task:Task就是主机上的一个进程,在大多数情况下,一个机器上只运行一个Task.

为什么Job是Task的集合呢? 在分布式深度学习框架中,我们一般把Job划分为Parameter和Worker,Parameter Job是管理参数的存储和更新工作.Worker Job是来运行ops.如果参数的数量太大,一台机器处理不了,这就要需要多个Tasks.
Cluster 是 Jobs 的集合: Cluster(集群),就是我们用的集群系统了

如何编写Task代码
首先,Task需要知道集群上都有哪些主机,以及它们都监听什么端口.tf.train.ClusterSpec()就是用来描述这个.

tf.train.ClusterSpec({
    "worker": [
        "worker_task0.example.com:2222",# /job:worker/task:0 运行的主机
        "worker_task1.example.com:2222",# /job:worker/task:1 运行的主机
        "worker_task2.example.com:2222"# /job:worker/task:3 运行的主机
    ],
    "ps": [
        "ps_task0.example.com:2222",  # /job:ps/task:0 运行的主机
        "ps_task1.example.com:2222"   # /job:ps/task:0 运行的主机
    ]})

然后,将ClusterSpec当作参数传入到 tf.train.Server()中,同时指定此Task的Job_name和task_index.

#jobName和taskIndex是函数运行时,通过命令行传递的参数
server = tf.train.Server(cluster, job_name=jobName, task_index=taskIndex)

首先,一个tf.train.Server包含了: 本地设备(GPUs,CPUs)的集合,可以连接到到其它task的ip:port(存储在cluster中), 还有一个session target用来执行分布操作.还有最重要的一点就是,它创建了一个服务器,监听port端口,如果有数据传过来,他就会在本地执行(启动session target,调用本地设备执行运算),然后结果返回给调用者.
tf.train.replica_device_setter(ps_tasks=0, ps_device='/job:ps', worker_device='/job:worker', merge_devices=True, cluster=None, ps_ops=None)),返回值可以被tf.device使用,指明下面代码中variable和ops放置的设备.
我们可以使用Supervisor帮助我们管理各个process.Supervisor的is_chief参数很重要,它指明用哪个task进行参数的初始化工作.sv.managed_session(server.target)创建一个被sv管理的session




 











